{"cells":[{"cell_type":"markdown","source":["#The Witcher on Azure: a classification problem using Natural Language Processing in Azure Databricks\n\nFor people who know me this will not come as a surprise, but two of my favorite topics to talk about are data and the Witcher. I have always been a huge fan of fantasy movies, games and books and the Witcher is easily one of my favorites. For readers who do not know what or who the Witcher is, the Witcher is a fantasy series by Polish writer Andrzej Sapkowski basically describing the adventures of monster hunter Geralt. Recently, a Netflix series based on these books has also been released, which I would definitely recommend. Describing this work of fiction will only be of secondary interest to this article, however. The main focus will reside with **Azure Databricks**.\n\nDatabricks is a **“Unified Data Analytics Platform”** which works together with cloud provider Azure to provide an online environment for data science using Apache Spark. In this platform, data scientists, engineers and analysts can come together to work on big data challenges. While the world of big data is multifaceted, and the possibilities Databricks offers are numerous, I want to narrow down the scope of the article to Machine Learning in Databricks. Specifically **Natural Language Processing (NLP)**.\n\nJohn Snow Labs, named after the English physician and not the Game of Thrones character, has developed an award winning open-source NLP library for Apache Spark. This framework can be easily integrated with Databricks, as they are both founded upon Apache Spark. This library offers a lot of out-of-the-box tools that are essential for NLP. For example, there is an in-built Entity Extractor, Tokenizer, Part of Speech Tagger, Named Entity Recognition and many more great features. They also offer pre-trained pipelines in multiple languages which allow you to identify words and sentences without having to spend (too much) effort in training a model yourself. This means you can instantly move on to the more interesting aspects of NLP. \n\nIn order to demonstrate some of these capabilities, I will perform a quick demonstration on some data I scraped from the internet. Using **R**, specifically the **rvest** library, I scraped the website witcher.fandom.com and extracted the character list containing all the characters that appear in the videogames. The wiki was particularly suited for a scraper, as all the pages follow the same layout. I performed some structuring and cleaning on the data in R, but I also left a block of raw text data that we will be using in this example.\n\nI exported the scraped data in CSV format to my local computer, but I could have moved it to storage in the cloud as well. For this you could use **Azure Data Lake Storage**, which is an easy to use, scalable data lake which is ideal for storing data such as a CSV file. On top of that, being part of the Azure environment it allows for an easy integration with Databricks. I would definitely recommend using this tool if you plan on working with large amounts of unstructured or semi-structured data.  \n\nThe problem that I want to tackle on Databricks is a classification problem. Specifically, based on the textual description (which is in free form) I want to classify a character as either a Dwarf or an Elf. This information could always come in handy when you have to decide whether or not the character could be tossed (yes, I’m also a huge fan of the Lord of the Rings). All silliness aside, the classification problem posed here can easily be transferred to other, more real-life scenarios. For example, you could measure and **predict the likelihood of a purchase or the attitude** towards a product a person has based on his or hers recent LinkedIn post or an email sent to your customer support. Similar techniques can readily be deployed in the development of **chatbots**. In fact, the model that I will show here is a simplified version of the model that I am actively developing for a job. What I want to show is that with this piece of technology, the **possibilities are endless**. \n\nNow, without further ado, I will demonstrate the code. In Databricks I will be using **Pyspark**, although Databricks also offers support for **R, Scala and SQL**. You can follow along with this code, even when you do not have a paid subscription to Databricks as there is also a free Community Edition available. For more information see: https://databricks.com/product/faq/community-edition\n\nFirst, we need to set up the environment by loading in the required libraries and read in the scraped data that I uploaded to the Databricks filestore in CSV format. I also perform a quick inspection of the data using the code below. This data is typical of what you can expect from scraped data, as there are lots of ill formatted field and missing data."],"metadata":{}},{"cell_type":"code","source":["# All functions needed to run this example\nimport sparknlp\n\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import CountVectorizer, IDF, StringIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import NaiveBayes\n\nimport pyspark.sql.functions as F"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":[" df = spark.read.csv(\"FileStore/tables/witcher_data-1.csv\",header=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["display(df.limit(3))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>name</th><th>race</th><th>gender</th><th>proffesion</th><th>affiliation</th><th>text</th></tr></thead><tbody><tr><td>1</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>NA</td><td>NULL</td></tr><tr><td>2</td><td>Abigail</td><td>Human</td><td>Female</td><td>NA</td><td>NA</td><td>Abigail was a witch who lived in her house in the outskirts of Vizima. She was not well-liked by the villagers and got blamed for many bad or strange occurrences. Her main skill was alchemy, so though the villagers approached her with suspicion, they also relied on her for potions and poisons - which she provided, rarely bothering with questions or moral objections.In the course of Chapter I, Abigail took in the orphaned Alvin after Geralt saved him from the barghests as Shani could not care for him from an inn. She also facilitated a trance where the boy revealed more about the beast and its minions who were plaguing the village.Geralt can buy a blade coating formula from her that makes it easier to battle specters and ghosts. In fact, she buys and sells quite a few things:If her fate was left to the villagers, she cursed Geralt in the name of the Lionhead Spider, which she called the Black Legba. Whether this meant she was also responsible for the misdeeds of the other villagers is unclear.</td></tr><tr><td>3</td><td>Adalbert</td><td>Human</td><td>Male</td><td>Soldier</td><td> Order of the Flaming Rose Vizima City Guard</td><td>Adalbert was a crossbowman of the City Guard who fought in the ranks of the Order of the Flaming Rose.He participated in fierce battles to take control of the Trade Quarter, resulting in the Order winning a Pyrrhic victory. A seriously wounded Adalbert went with his remaining strength to the Cloister of the Flaming Rose in the Temple Quarter. Being in a state of agony, he informed Jacques de Aldersberg what happened, and then told the Master about the death of Roderick de Wett caused by Geralt. The Grand Master then thanked the soldier for his faithful service, just before Adalbert died from his wounds.</td></tr></tbody></table></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["The data that I will be using for the classification is the text column and the race column. I take a subset of the data so that only Dwarf and Elf (Aen Seidhe) characters are present. The fact that these are the most frequenly occurring classes, apart from humans, had nothing to do with my choice for these classes *ahum*. In all fairness, the number of observations is quite low, and in a real-life situation you definitely want more. For illustration purposes however, this is fine."],"metadata":{}},{"cell_type":"code","source":["subset = df.where(df.race.isin([\"Elf (Aen Seidhe)\", \"Dwarf\"]))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["display(subset.limit(3))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>name</th><th>race</th><th>gender</th><th>proffesion</th><th>affiliation</th><th>text</th></tr></thead><tbody><tr><td>24</td><td>Chireadan</td><td>Elf (Aen Seidhe)</td><td>Male</td><td>Tavern owner (canon)Guerrilla fighter (games)</td><td> Scoia'tael (games)</td><td>Chireadan was an elf from the Redanian city of Rinde, a tavern owner, and Errdil's cousin.Despite elves typically not finding humans to be attractive, he was secretly in love with Yennefer, though he never revealed his feelings to the sorceress. However, he didn't let his feelings get in the way when he saw that Geralt was also infatuated with Yennefer and even pulled the others away when he saw the two having sex in his cousin's destroyed inn after fighting off a djinn.In Chapter IV, at the Lakeside, Chireadan, an elf among Toruviel's ragtag group recognizes Geralt. It seems he knows the witcher from somewhere before, but of course Geralt has no memory of him. This previous encounter, which is not described in any further detail, is a reference to the The Last Wish.He is also a sharper, and thus one of the available dice players. He can be found any time, day or night to play. During the day, he is typically sitting at one of the two campfires nearest the Elven Cave. At night, he will be sleeping in the cave, but does not complain at all about being woken up just for a game.</td></tr><tr><td>38</td><td>Elven craftsman</td><td>Elf (Aen Seidhe)</td><td>Male</td><td>Master craftsman</td><td> Toruviel</td><td>The elven craftsman was part of Toruviel's band of starving elves who camped in the cave by the Lakeside in Murky Waters. He was looking for four pieces of centipede armor for his work and was willing to pay. He was also a master craftsman, though the term slightly offended his artistic nature, and was capable of doing many things: mirror reassembly being one such skill.He was well versed in the history of Raven's armor.</td></tr><tr><td>53</td><td>Golan Vivaldi</td><td>Dwarf</td><td>Male</td><td>Banker</td><td> Vivaldi Bank (Vizima branch)</td><td>\"Golan Vivaldi is a dwarf, and part of the \"\"Vivaldi family\"\"</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["I previously mentioned that there are pre-trained pipelines available. Here, I chose to manually set the stages instead of using the pre-trained pipeline, so you can get a better understanding of what is going on. The data that we have has to undergo a number of changes before we can actually use it. There are some steps shown here that are not strictly necessary for this problem, but could serve to illustrate some additional capabilities. \n\nThe stages that the character text have to go through are **document assembler, sentence detector, tokenizer, stop words cleaner, normalizer, lemmatizer, finisher, countvectorizer, idf, and an indexer**. The first few stages break up the text in individual parts, remove unnecessary words and normalize the remaining words. An example of this is transforming all the same words with the same stem but written in different tenses to one and the same tense. \n\nIn order to use the text to classify the characters, it has to be in a specific format, namely a **vector**. Furthermore, I want to make a count of how many times certain words appear in the text. This can be done with the count vectorizer. For additional information, I also determine the **term frequency-inverse document frequency (TF-IDF)**. This basically lowers the importance of words that appear in every entry. For example, given that all the characters are part of the Witcher universe, the word witcher will likely appear quite frequently. This does not mean, however, that this is very useful word to predict with as it can be applied to all entries. The TF-IDF score account for this fact. Finally, the indexer step translates the values for race to numbers. This is because predictions can only be performed on numbers. All these steps together transform the data in something that we can use to make the prediction. \n\nNote that in the code below I manually set a number of common stop words to be removed."],"metadata":{}},{"cell_type":"code","source":["words_to_remove_list = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["document_assembler = DocumentAssembler() \\\n    .setInputCol(\"text\") \\\n    .setOutputCol(\"document\")\n    \nsentence_detector = SentenceDetector() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentence\") \\\n    .setUseAbbreviations(True)\n    \ntokenizer = Tokenizer() \\\n  .setInputCols([\"sentence\"]) \\\n  .setOutputCol(\"token\")\n\nstop_words_cleaner = StopWordsCleaner() \\\n        .setInputCols([\"token\"]) \\\n        .setOutputCol(\"cleanTokens\") \\\n        .setCaseSensitive(False) \\\n        .setStopWords(words_to_remove_list)\n    \nnormalizer = Normalizer() \\\n    .setInputCols([\"cleanTokens\"]) \\\n    .setOutputCol(\"normalized\")\n\nlemmatizer = LemmatizerModel.pretrained(name='lemma', lang='nl') \\\n     .setInputCols(['normalized']) \\\n     .setOutputCol('lemma')\n\nfinisher = Finisher() \\\n    .setInputCols([\"lemma\"]) \\\n    .setOutputCols([\"ntokens\"]) \\\n    .setOutputAsArray(True) \\\n    .setCleanAnnotations(False) \ncountvectorizer = CountVectorizer(inputCol=\"ntokens\", outputCol=\"features\", minDF = 3.0)\n\nidf = IDF(inputCol=\"features\", outputCol=\"features_updated\")\n\nindexer = StringIndexer(inputCol=\"race\", outputCol=\"raceIndex\")\n\nnlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, stop_words_cleaner, normalizer, lemmatizer, finisher, countvectorizer, idf, indexer])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">lemma download started this may take some time.\nApproximate size to download 285.9 KB\n\r[ | ]\r[ / ]\r[OK!]\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Before we move to predict anything, I first divide the data in a training and test subset. This gives a better assessment of the performance of the model that we are going to use. Databricks offers many models that can be readily applied. In this case, since the outcome variable is binary, there are a lot of options we can choose from. For example, **Decision trees, logistic regression or Naïve Bayes** are all models which can be applied to this situation. In this case, I chose to use Naïve Bayes as it has been successful for me in similar cases before."],"metadata":{}},{"cell_type":"code","source":["processed_subset = nlp_pipeline.fit(subset).transform(subset)\n\n(trainingData, testData) = processed_subset.randomSplit([0.8, 0.2],seed = 11)\n\nnb = NaiveBayes(modelType=\"multinomial\",labelCol=\"raceIndex\", featuresCol=\"features_updated\")\nnbModel = nb.fit(trainingData)\nnb_predictions = nbModel.transform(testData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Now that we have applied the model, we can evaluate the performance. In order to assess this, I look at the **f1 score** which is the harmonic mean of the precision and recall. In this case, we obtain a score of .90 which is pretty good!"],"metadata":{}},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"raceIndex\", predictionCol=\"prediction\", metricName=\"f1\")\nnb_accuracy = evaluator.evaluate(nb_predictions)\nprint(\"F1 score of NaiveBayes is = %g\"% (nb_accuracy))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">F1 score of NaiveBayes is = 0.905628\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Using this model, we were able to make a pretty good distinction between a Dwarf and an Elf. Of course, this model can be further improved and more data should be added in order to obtain better predictions. However, what I have shown here are some of the basic steps and capabilities Databricks offers in terms of NLP and Machine Learning.\n\nTo summarize, Azure Databricks offers an easy to use data analytics platform in the cloud. It is able to ingest data from multiple sources, such as a data lake, and apply machine learning on this data. The possibilies in this regard are endless, and in this example I gave a quick demonstration of how to use unstructured text data and use it to determine a fanatasy characters' race using NLP. \n\nIf you are curious about other possibilies Databricks could offer you or you are intrigued by this article please let me know. I am always eager to discuss these topics with interested readers. Also, if you have opportunities or resources for me to expand my knowledge regarding this topic, do not hestitate to contact me! I am also always up for a round of Gwent ;)"],"metadata":{}}],"metadata":{"name":"The Witcher on Azure","notebookId":1507006614546584},"nbformat":4,"nbformat_minor":0}
